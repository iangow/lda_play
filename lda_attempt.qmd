---
title: "Topic Modeling of Earnings Calls using LDA"
format: html
editor: 
  markdown: 
    wrap: sentence
---

This material is based on material found [here](https://highdemandskills.com/topic-modeling-lda/#h3-2).

## Set up your computer

A number of packages are required.
These can be installed using `conda`:

    conda install nbformat jupyter spacy gensim wordcloud matplotlib

Use `pip` (e.g., `pip install wordcloud`) if necessary.
I found I needed to run `conda install matplotlib --update-all` to get `matplotlib` to work with Quarto.

The code also requires a large file (`en_core_web_lg`) from Spacy:

    python -m spacy download en_core_web_lg    

## Importing libraries

We first import libraries for get earnings call transcripts (`pandas` and `sqlalchemy`), text pre-processing (`SpaCy`), displaying results (`matplotlib`, `pprint` and `wordcloud`) and LDA (`gensim`).

```{python}
import pandas as pd
from pandas import to_datetime
from sqlalchemy import create_engine
import matplotlib.pyplot as plt

from gensim.corpora import Dictionary
from gensim.models import LdaModel, CoherenceModel

import spacy
from pprint import pprint
from wordcloud import WordCloud

```

```{python}
nlp = spacy.load("en_core_web_lg")

# In case max_length is set to lower than this
# (ensures sufficient memory)
nlp.max_length = 1500000 
```

## Sourcing earnings call transcripts

I will use the same tickers as are used on the [original post](https://highdemandskills.com/topic-modeling-lda/#h3-2), but will get a call from 2019 (we don't have data for 2021 in most cases) and get the data from the MCCGR database.

```{python}
import re

call_list = ['ADSK-Q2-2021', 'ANF-Q2-2020', 'APPEF-Q2-2020', 
             'BBY-Q2-2020', 'CRM-Q2-2021', 'DELL-Q2-2021',
             'DE-Q3-2020', 'DG-Q2-2020', 'DLTR-Q2-2020', 'EL-Q4-2020',
             'EV-Q3-2020', 'FLWS-Q4-2020', 'GPS-Q2-2020',
             'HPQ-Q3-2020', 'INTU-Q4-2020', 'JWN-Q2-2020', 
             'MRVL-Q2-2021', 'NVDA-Q2-2021', 'SPLK-Q2-2021', 
             'TD-Q3-2020', 'TOL-Q3-2020', 'TSLA-Q2-2020', 
             'VMW-Q2-2021', 'WDAY-Q2-2021',  'TGT-Q2-2020',
             'BJ-Q2-2020', 'A-Q3-2020', 'HD-Q2-2020',
             'KSS-Q2-2020', 'ADMP-Q2-2020', 'FL-Q2-2020', 
             'GASS-Q2-2020', 'ADI-Q3-2020', 'WMT-Q2-2021', 
             'CODX-Q2-2020', 'ECC-Q2-2020']

tickers = [call.split("-") for call in call_list]
```

I get the list of calls and store them in `calls`.

```{python}
engine = create_engine("postgresql://10.101.13.99/crsp")

def get_call(doc):
    sql_text = """
        SET TIME ZONE 'UTC';
       
        SELECT *
        FROM streetevents.calls
        WHERE company_ticker = '%s' AND 
            event_type = 1 AND
            date_part('year', start_date)='2019'
        LIMIT 1;
        """ % doc[0]
        
    df = pd.read_sql(sql_text, engine)
    
    return df

calls = pd.concat(list(map(get_call, tickers)))
calls.reset_index(drop=True, inplace=True)
print(calls[['file_name', 'company_ticker', 'start_date']])
```

## Text pre-processing

Following the original source, I'll keep nouns, adjectives, and verbs---this seems to work well.

You can learn more about text pre-processing as a part of the natural language processing workflow [here](https://highdemandskills.com/natural-language-processing-explained-simply/#h2-3).

I prepare each transcript in turn, by looping through `calls` and collecting the results in a list.
I create `e_call_docs`, a list of lists, made up of the transcripts forming the training corpus, with each transcript being a list of words.

```{python}
def get_speaker_data(call):
    
    sql_get_call_text = """
        SET TIME ZONE 'UTC';
        
        SELECT file_name, last_update, section, 
            speaker_number, speaker_text
        FROM streetevents.speaker_data
        WHERE context='qa' AND file_name='%s' AND last_update = '%s'
        
    """ % (call['file_name'], call['last_update'])
    
    df = pd.read_sql(sql_get_call_text, engine)
    df['last_update'] = df['last_update'].apply(lambda d: to_datetime(str(d)))
    return df
```

```{python}
def get_call_text(call):
    df = get_speaker_data(call)
    return " ".join(df['speaker_text'])
```

```{python}
def parse_call(call):

    # Parse document with SpaCy
    e_call = nlp(get_call_text(call))
    
    # Further cleaning and selection of text characteristics
    # Retain words that are not a stop word nor punctuation, 
    # and only if a noun, adjective or verb
    # Convert to lower case and retain the lemmatized version 
    # of the word (a string).
    return [token.lemma_.lower() for token in e_call
        if not token.is_stop and not token.is_punct and 
            token.pos_ in ["NOUN", "ADJ", "VERB"]]
```

Build the training corpus 'list of lists'

```{python}
e_call_docs = [parse_call(row) for idx, row in calls.iterrows()]
```

## Inspecting the training corpus

It's helpful to have an idea of what the training corpus looks like---there are various ways to do this, including word clouds.
The original code had a separate list for creating the word cloud, whereas I just use the original list of lists and the `flatten_to_strings` function I supplied [here](https://stackoverflow.com/questions/17864466/flatten-a-list-of-strings-and-lists-of-strings-and-lists-in-python).

```{python}
def flatten_to_strings(listOfLists):
    """Flatten a list of (lists of (lists of strings)) for 
    any level of nesting"""
    result = []
    
    for i in listOfLists:
        # Only append if i is a string
        if isinstance(i, str):
            result.append(i)
        # Otherwise call this function recursively
        else:
            result.extend(flatten_to_strings(i))
    return result
```

```{python}
ECallWordCloud = flatten_to_strings(e_call_docs)

# Generate and plot WordCloud for full training corpus
wordcloud = WordCloud(background_color="white").generate(','.join(ECallWordCloud)) # NB. 'join' method used to convert the documents list to text format
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
```

The word cloud of the training corpus shows a number of words that you might expect to see in company earnings calls --- words related to customers, earnings periods ("year"), financials ("continue" and "growth"), and words to express opinions ("think" and "expect").

## Training the LDA model

To train our LDA model, first, form a dictionary by mapping the training corpus to word IDs.
Then, convert the words in each transcript to numbers (text representation) using a bag-of-words approach.

Word IDs for training corpus

```{python}
# Map training corpus to IDs
ID2word = Dictionary(e_call_docs)

# Convert all transcripts in training corpus using bag-of-words
train_corpus = [ID2word.doc2bow(doc) for doc in e_call_docs]
```

We use the `gensim` package to generate our LDA model.
This requires training the model using our training corpus and selecting the number of topics as an input.
Since we don't know how many topics are likely to emerge from the training corpus, let's start with 5.

```{python}
num_topics = 5 # Set number of topics

# Train LDA model on the training corpus
lda_model = LdaModel(corpus=train_corpus, num_topics=num_topics, 
                     id2word=ID2word, passes=100)
```

The passes flag refers to the number of iterations through the corpus during training---the higher, the better for defining topics, albeit at the cost of extra processing time.
Following the original page, I set the flag to `passes=100` to produce better results.

I did not use the `LdaMulticore` version of the model, which makes use of parallelization for faster processing, because I encountered the issue described [here](https://github.com/RaRe-Technologies/gensim/issues/2223).

### Observing the topics

You can observe the key words in each topic that results from the training.

```{python}
# Print topics generated from the training corpus
pprint(lda_model.print_topics(num_words=4))
```

Use `pprint` to print in an easier-to-read format---the above code prints the top 4 keywords in each of the 5 topics generated through training: Also shown is the frequency of each keyword (the decimal number next to each keyword).

How good are these topics?

To answer this question, let's evaluate our model results.

## Model evaluation

There are several ways to evaluate LDA models, and they're not all based on numbers.
Context has a role to play, as does the practical usefulness of the generated topics.

The considerations and challenges of topic model evaluation are discussed further in [this article](https://highdemandskills.com/topic-model-evaluation/).

In terms of quantitative measures, a common way to evaluate LDA models is through the coherence score.

The coherence score of an LDA model measures the degree of semantic similarity between words in each topic.

All else equal, a higher coherence score is better, as it indicates a higher degree of likeness in the meaning of the words within each topic.

We can measure our model's coherence using the CoherenceModel within gensim.

### Calculating model coherence

```{python}
# Set up coherence model
cm = CoherenceModel(model=lda_model, texts=e_call_docs, 
                    dictionary=ID2word, coherence='c_v', processes = 1)
```

```{python}
# Calculate and print coherence
coherence_lda = cm.get_coherence()
print('-'*50)
print('\nCoherence Score:', coherence_lda)
print('-'*50)
```

We choose coherence='c_v' as our coherence method, which has been shown to be effective and is a popular choice.
Using this method, coherence scores between 0 and 1 are typical.
